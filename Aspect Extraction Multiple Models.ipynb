{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file contains following models:\n",
    "#### - CNN + WE + POS\n",
    "#### - LSTM\n",
    "#### - Evaluation of CNN+WE, CNN+WE+POS, LSTM\n",
    "#### - CNN + WE + POS + Window of size 5\n",
    "#### -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dependencies and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GT 730M (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Generated list of sentences..\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "print('Processing text dataset')\n",
    "\n",
    "tree = ET.parse(\"/home/jeet/Academics/CS671/Project/Restaurants_Train.xml\")\n",
    "corpus = tree.getroot()\n",
    "sentences = [] # List of list of sentences.\n",
    "sent = corpus.findall('.//sentence')\n",
    "for s in sent:\n",
    "    sentences.append(s.find('text').text)\n",
    "\n",
    "print ('Generated list of sentences..')\n",
    "\n",
    "MAX_SEQ_LENGTH = 69\n",
    "MAX_NB_WORDS = 40000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vectorize the text samples into a 2D integer tensor and padding the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5250 unique tokens.\n",
      "Let's have a quick look at the word_index data..\n",
      "Shape of data tensor: (3044, 71)\n",
      "[202   1  79   7  36 641   4  86   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "Shape of data tensor: (3044, 73)\n",
      "[  0   0 202   1  79   7  36 641   4  86   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, lower=False)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print (\"Let's have a quick look at the word_index data..\")\n",
    "\n",
    "# Here padding has been done at both the ends since we will need to take the context window size of 4 units.\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH+2, padding='post')\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print (data[0])\n",
    "data = pad_sequences(data, maxlen=MAX_SEQ_LENGTH+4, padding='pre')\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print (data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: (3044, 69)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: FutureWarning: The behavior of this method will change in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "raw_output = corpus.findall('.//sentence')\n",
    "train_out= np.zeros(shape=(3044,69))\n",
    "i=0\n",
    "for output in raw_output:\n",
    "    s = text_to_word_sequence(output.find('text').text, lower=False)\n",
    "    indices = np.zeros(MAX_SEQ_LENGTH)\n",
    "    \n",
    "    aspectTerms = output.find('aspectTerms')\n",
    "    if (aspectTerms):\n",
    "        aspectTerm = aspectTerms.findall('aspectTerm')\n",
    "        if (aspectTerm):\n",
    "            for aspect_term in aspectTerm:\n",
    "                try:\n",
    "                    indices[s.index(aspect_term.attrib['term'])] = 1\n",
    "#                     print (indices)\n",
    "                except:\n",
    "                    continue\n",
    "    train_out[i] = indices\n",
    "    i=i+1\n",
    "\n",
    "print (\"Shape of output tensor:\", train_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Embedding Layer set..\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words = len(word_index)\n",
    "embedding_matrix = np.zeros((nb_words + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(nb_words + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQ_LENGTH+4,\n",
    "                            trainable=False)\n",
    "print('Embedding Layer set..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated word Embeddings..\n",
      "Shape of Embedding_output (3044, 73, 300)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "embedding_model = Sequential()\n",
    "embedding_model.add(embedding_layer)\n",
    "\n",
    "embedding_model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer='rmsprop',\n",
    "                        metrics=['acc']\n",
    "                       )\n",
    "embedding_output = embedding_model.predict(data)\n",
    "print('Generated word Embeddings..')\n",
    "print('Shape of Embedding_output', embedding_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding POS-tag features to input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Word-Embeddings and POS Tag Features..\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_input = np.zeros(shape=(3044,73,306))\n",
    "le = preprocessing.LabelEncoder()\n",
    "tags = [\"CC\",\"NN\",\"JJ\",\"VB\",\"RB\",\"IN\"]\n",
    "le.fit(tags)\n",
    "i=0\n",
    "sentences = corpus.findall('.//sentence')\n",
    "for sent in sentences:\n",
    "    s = text_to_word_sequence(sent.find('text').text)\n",
    "    tags_for_sent = nltk.pos_tag(s)\n",
    "    sent_len = len(tags_for_sent)\n",
    "    ohe = [0]*6\n",
    "        69\n",
    "    for j in xrange(69):\n",
    "        if j< len(tags_for_sent) and tags_for_sent[j][1][:2] in tags:\n",
    "            ohe[le.transform(tags_for_sent[j][1][:2])] = 1\n",
    "        train_input[i][j] = np.concatenate([embedding_output[i][j],ohe])\n",
    "    i=i+1\n",
    "    \n",
    "print('Concatenated Word-Embeddings and POS Tag Features..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redfining the input to incorporate the window of size 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3044, 73, 306)\n"
     ]
    }
   ],
   "source": [
    "print (train_input.shape)\n",
    "window_size = 5\n",
    "final_input = np.zeros(shape=(3044,69,306,5))\n",
    "for i in xrange(train_input.shape[0]):      # Access each sentence representation\n",
    "    for j in xrange(train_input.shape[1]):      # Access embedded representation of each word\n",
    "        if j>1 and j<71:\n",
    "            for k in xrange(train_input.shape[2]):    # Access vector\n",
    "#                 print (train_input[i][j-2][k],train_input[i][j-1][k],train_input[i][j][k],train_input[i][j+1][k],train_input[i][j+2][k])\n",
    "                final_input[i][j-2][k] = np.array([train_input[i][j-2][k],train_input[i][j-1][k],train_input[i][j][k],train_input[i][j+1][k],train_input[i][j+2][k]],dtype=float)\n",
    "\n",
    "print('Window features concatenated..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WE + POS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution1D, Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling1D, MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(100, 5, border_mode=\"same\", input_shape=(69, 306)))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(MaxPooling1D(pool_length=5))\n",
    "model.add(Convolution1D(50, 3, border_mode=\"same\"))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation(\"tanh\"))\n",
    "# softmax classifier\n",
    "model.add(Dense(69, W_regularizer=l2(0.01)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.load_weights('aspect_model_wepos.h5')\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['acc'])\n",
    "\n",
    "# print('Model Trained..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2739 samples, validate on 305 samples\n",
      "Epoch 1/50\n",
      "2739/2739 [==============================] - 6s - loss: 2.9180 - acc: 0.2172 - val_loss: 2.2590 - val_acc: 0.4066\n",
      "Epoch 2/50\n",
      "2739/2739 [==============================] - 5s - loss: 2.1496 - acc: 0.3852 - val_loss: 2.0137 - val_acc: 0.5246\n",
      "Epoch 3/50\n",
      "2739/2739 [==============================] - 5s - loss: 1.6980 - acc: 0.4673 - val_loss: 1.8792 - val_acc: 0.4066\n",
      "Epoch 4/50\n",
      "2739/2739 [==============================] - 5s - loss: 1.4125 - acc: 0.5016 - val_loss: 1.8505 - val_acc: 0.3967\n",
      "Epoch 5/50\n",
      "2739/2739 [==============================] - 5s - loss: 1.2778 - acc: 0.5016 - val_loss: 1.7848 - val_acc: 0.4689\n",
      "Epoch 6/50\n",
      "2739/2739 [==============================] - 5s - loss: 1.1948 - acc: 0.5046 - val_loss: 1.6736 - val_acc: 0.4393\n",
      "Epoch 7/50\n",
      "2739/2739 [==============================] - 5s - loss: 1.1497 - acc: 0.5075 - val_loss: 1.7521 - val_acc: 0.3934\n",
      "Epoch 8/50\n",
      "2739/2739 [==============================] - 5s - loss: 1.0914 - acc: 0.5057 - val_loss: 1.6744 - val_acc: 0.4557\n",
      "Epoch 9/50\n",
      "2739/2739 [==============================] - 5s - loss: 1.0566 - acc: 0.5152 - val_loss: 1.6572 - val_acc: 0.4820\n",
      "Epoch 10/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.0176 - acc: 0.5104 - val_loss: 1.6509 - val_acc: 0.4426\n",
      "Epoch 11/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9974 - acc: 0.5108 - val_loss: 1.6187 - val_acc: 0.4525\n",
      "Epoch 12/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9702 - acc: 0.5049 - val_loss: 1.6180 - val_acc: 0.4492\n",
      "Epoch 13/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.9540 - acc: 0.5005 - val_loss: 1.6547 - val_acc: 0.4557\n",
      "Epoch 14/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9311 - acc: 0.5038 - val_loss: 1.6157 - val_acc: 0.4328\n",
      "Epoch 15/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.9272 - acc: 0.5100 - val_loss: 1.6350 - val_acc: 0.4590\n",
      "Epoch 16/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.9049 - acc: 0.5108 - val_loss: 1.5123 - val_acc: 0.4262\n",
      "Epoch 17/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.9011 - acc: 0.5100 - val_loss: 1.6004 - val_acc: 0.4492\n",
      "Epoch 18/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8773 - acc: 0.5071 - val_loss: 1.5866 - val_acc: 0.4426\n",
      "Epoch 19/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8888 - acc: 0.4995 - val_loss: 1.6062 - val_acc: 0.4820\n",
      "Epoch 20/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8653 - acc: 0.5046 - val_loss: 1.6539 - val_acc: 0.4852\n",
      "Epoch 21/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8603 - acc: 0.5071 - val_loss: 1.6015 - val_acc: 0.5016\n",
      "Epoch 22/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8585 - acc: 0.4984 - val_loss: 1.6143 - val_acc: 0.4721\n",
      "Epoch 23/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8490 - acc: 0.4976 - val_loss: 1.5396 - val_acc: 0.4295\n",
      "Epoch 24/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8476 - acc: 0.5024 - val_loss: 1.5647 - val_acc: 0.4689\n",
      "Epoch 25/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8311 - acc: 0.4991 - val_loss: 1.5706 - val_acc: 0.4820\n",
      "Epoch 26/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8344 - acc: 0.5024 - val_loss: 1.5561 - val_acc: 0.4328\n",
      "Epoch 27/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8211 - acc: 0.4980 - val_loss: 1.5780 - val_acc: 0.4918\n",
      "Epoch 28/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8223 - acc: 0.5152 - val_loss: 1.5623 - val_acc: 0.4984\n",
      "Epoch 29/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8178 - acc: 0.4987 - val_loss: 1.6024 - val_acc: 0.4787\n",
      "Epoch 30/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8147 - acc: 0.4984 - val_loss: 1.5754 - val_acc: 0.4689\n",
      "Epoch 31/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8150 - acc: 0.5042 - val_loss: 1.5912 - val_acc: 0.4721\n",
      "Epoch 32/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8064 - acc: 0.4980 - val_loss: 1.5929 - val_acc: 0.4852\n",
      "Epoch 33/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8059 - acc: 0.4965 - val_loss: 1.6143 - val_acc: 0.4918\n",
      "Epoch 34/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8038 - acc: 0.5057 - val_loss: 1.6249 - val_acc: 0.4557\n",
      "Epoch 35/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8061 - acc: 0.5148 - val_loss: 1.5854 - val_acc: 0.4721\n",
      "Epoch 36/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7990 - acc: 0.4889 - val_loss: 1.6113 - val_acc: 0.4328\n",
      "Epoch 37/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7953 - acc: 0.4925 - val_loss: 1.6174 - val_acc: 0.4885\n",
      "Epoch 38/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7963 - acc: 0.4940 - val_loss: 1.5828 - val_acc: 0.4623\n",
      "Epoch 39/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7951 - acc: 0.5082 - val_loss: 1.6131 - val_acc: 0.5016\n",
      "Epoch 40/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7889 - acc: 0.5089 - val_loss: 1.6308 - val_acc: 0.4787\n",
      "Epoch 41/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7914 - acc: 0.4936 - val_loss: 1.6151 - val_acc: 0.4328\n",
      "Epoch 42/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7939 - acc: 0.5141 - val_loss: 1.5720 - val_acc: 0.4656\n",
      "Epoch 43/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7883 - acc: 0.5009 - val_loss: 1.6166 - val_acc: 0.4984\n",
      "Epoch 44/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7836 - acc: 0.5097 - val_loss: 1.5537 - val_acc: 0.5148\n",
      "Epoch 45/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7904 - acc: 0.5031 - val_loss: 1.5565 - val_acc: 0.4852\n",
      "Epoch 46/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7882 - acc: 0.5002 - val_loss: 1.6075 - val_acc: 0.5082\n",
      "Epoch 47/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7850 - acc: 0.5115 - val_loss: 1.5676 - val_acc: 0.4951\n",
      "Epoch 48/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7833 - acc: 0.5089 - val_loss: 1.5472 - val_acc: 0.4852\n",
      "Epoch 49/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7847 - acc: 0.5078 - val_loss: 1.5741 - val_acc: 0.4852\n",
      "Epoch 50/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7821 - acc: 0.4969 - val_loss: 1.5414 - val_acc: 0.4426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f80fcfa8f10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_input, train_out,\n",
    "          validation_split=0.1,\n",
    "          batch_size=10,\n",
    "          nb_epoch=50\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('aspect_model_wepos.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import TimeDistributedDense\n",
    "from keras.layers import Activation\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(output_dim=306,input_dim=306,return_sequences=True,activation='sigmoid',inner_activation='hard_sigmoid'))\n",
    "lstm_model.add(LSTM(output_dim=306,input_dim=306,activation='sigmoid',inner_activation='hard_sigmoid'))\n",
    "lstm_model.add(Activation('hard_sigmoid'))\n",
    "\n",
    "lstm_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2739 samples, validate on 305 samples\n",
      "Epoch 1/5\n",
      "2739/2739 [==============================] - 47s - loss: 3.6242 - acc: 0.4830 - val_loss: 3.6045 - val_acc: 0.4721\n",
      "Epoch 2/5\n",
      "2739/2739 [==============================] - 47s - loss: 3.6240 - acc: 0.4830 - val_loss: 3.6048 - val_acc: 0.4721\n",
      "Epoch 3/5\n",
      "2739/2739 [==============================] - 45s - loss: 3.6241 - acc: 0.4830 - val_loss: 3.6044 - val_acc: 0.4721\n",
      "Epoch 4/5\n",
      "2739/2739 [==============================] - 46s - loss: 3.6240 - acc: 0.4794 - val_loss: 3.6053 - val_acc: 0.1607\n",
      "Epoch 5/5\n",
      "2739/2739 [==============================] - 46s - loss: 3.6242 - acc: 0.4794 - val_loss: 3.6045 - val_acc: 0.4721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcbad1e3ed0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(train_input,train_out,\n",
    "              validation_split=0.1,\n",
    "              nb_epoch=5\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_model.save_weights('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(train_input[2739:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n"
     ]
    }
   ],
   "source": [
    "print (y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN+WE+POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "processed_output = []\n",
    "for i in xrange(y_pred.shape[0]):\n",
    "    processed_label =[]\n",
    "    for j in xrange(y_pred.shape[1]):\n",
    "        if y_pred[i][j] > 0.5:\n",
    "            processed_label.append(1)\n",
    "        else:\n",
    "            processed_label.append(0)\n",
    "    processed_output.append(processed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = train_out[2739:]\n",
    "total_pos = 0.0\n",
    "true_pos = 0.0\n",
    "total_neg = 0.0\n",
    "true_neg = 0.0\n",
    "for i in xrange(test_data.shape[0]):\n",
    "    for j in xrange(test_data.shape[1]):\n",
    "        if test_data[i][j] == 1:\n",
    "            total_pos += 1\n",
    "            if processed_output[i][j] ==1:\n",
    "                true_pos +=1\n",
    "        if test_data[i][j] == 0:\n",
    "            total_neg += 1\n",
    "            if processed_output[i][j] ==0:\n",
    "                true_neg += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "false_pos = total_neg-true_neg\n",
    "false_neg = total_pos-true_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.534562, recall 0.426471, accuracy 0.987788\n"
     ]
    }
   ],
   "source": [
    "print (\"precision %f, recall %f\" % (true_pos/(true_pos+false_pos), true_pos/total_pos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN+WE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Convolution1D(100, 5, border_mode=\"same\", input_shape=(69, 300)))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(MaxPooling1D(pool_length=5))\n",
    "model.add(Convolution1D(50, 3, border_mode=\"same\"))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation(\"tanh\"))\n",
    "# softmax classifier\n",
    "model.add(Dense(69, W_regularizer=l2(0.01)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.load_weights('aspect_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(data[2739:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_output = []\n",
    "for i in xrange(y_pred.shape[0]):\n",
    "    processed_label =[]\n",
    "    for j in xrange(y_pred.shape[1]):\n",
    "        if y_pred[i][j] > 0.5:\n",
    "            processed_label.append(1)\n",
    "        else:\n",
    "            processed_label.append(0)\n",
    "    processed_output.append(processed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = train_out[2739:]\n",
    "total_pos = 0.0\n",
    "true_pos = 0.0\n",
    "total_neg = 0.0\n",
    "true_neg = 0.0\n",
    "for i in xrange(test_data.shape[0]):\n",
    "    for j in xrange(test_data.shape[1]):\n",
    "        if test_data[i][j] == 1:\n",
    "            total_pos += 1\n",
    "            if processed_output[i][j] ==1:\n",
    "                true_pos +=1\n",
    "        if test_data[i][j] == 0:\n",
    "            total_neg += 1\n",
    "            if processed_output[i][j] ==0:\n",
    "                true_neg += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_pos = total_neg-true_neg\n",
    "false_neg = total_pos-true_pos\n",
    "\n",
    "print (\"precision %f, recall %f\" % (true_pos/(true_pos+false_pos), true_pos/total_pos)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
