{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GT 730M (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Generated list of sentences..\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "print('Processing text dataset')\n",
    "\n",
    "tree = ET.parse(\"/home/jeet/Academics/CS671/Project/Restaurants_Train.xml\")\n",
    "corpus = tree.getroot()\n",
    "sentences = [] # List of list of sentences.\n",
    "sent = corpus.findall('.//sentence')\n",
    "for s in sent:\n",
    "    sentences.append(s.find('text').text)\n",
    "\n",
    "print ('Generated list of sentences..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 69\n",
    "MAX_NB_WORDS = 40000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vectorize the text samples into a 2D integer tensor and padding the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5250 unique tokens.\n",
      "Let's have a quick look at the word_index data..\n",
      "[('rasamalai', 2402), ('limited', 647), ('arrives', 4234), ('legacies', 2403), ('raining', 2404), ('saves', 1617), ('AN', 2405), ('meatsauce', 4956), ('sleek', 1237), ('four', 648)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, lower=False)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print (\"Let's have a quick look at the word_index data..\")\n",
    "print (list(word_index.items())[:10])\n",
    "# print (word_index['limited'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (3044, 69)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### defining output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:11: FutureWarning: The behavior of this method will change in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "raw_output = corpus.findall('.//sentence')\n",
    "train_out= np.zeros(shape=(3044,69))\n",
    "i=0\n",
    "for output in raw_output:\n",
    "    s = text_to_word_sequence(output.find('text').text, lower=False)\n",
    "    indices = np.zeros(MAX_SEQ_LENGTH)\n",
    "    \n",
    "    aspectTerms = output.find('aspectTerms')\n",
    "    if (aspectTerms):\n",
    "        aspectTerm = aspectTerms.findall('aspectTerm')\n",
    "        if (aspectTerm):\n",
    "            for aspect_term in aspectTerm:\n",
    "                try:\n",
    "                    indices[s.index(aspect_term.attrib['term'])] = 1\n",
    "#                     print (indices)\n",
    "                except:\n",
    "                    continue\n",
    "    train_out[i] = indices\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3044, 69)\n"
     ]
    }
   ],
   "source": [
    "print (train_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words = len(word_index)\n",
    "embedding_matrix = np.zeros((nb_words + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(nb_words + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQ_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution1D, Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling1D, MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Convolution1D(100, 5, border_mode=\"same\", input_shape=(65, 300)))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(MaxPooling1D(pool_length=5))\n",
    "model.add(Convolution1D(50, 3, border_mode=\"same\"))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation(\"tanh\"))\n",
    "# softmax classifier\n",
    "model.add(Dense(69, W_regularizer=l2(0.01)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "\n",
    "# # train a 1D convnet with global maxpooling\n",
    "# sequence_input = Input(shape=(MAX_SEQ_LENGTH,), dtype='int32')\n",
    "# embedded_sequences = embedding_layer(sequence_input)\n",
    "# x = Conv1D(100, 5, activation='tanh')(embedded_sequences)\n",
    "# x = MaxPooling1D(5)(x)\n",
    "# x = Conv1D(50, 5, activation='relu')(x)\n",
    "# x = MaxPooling1D(5)(x)\n",
    "# x = Conv1D(128, 5, activation='relu')(x)\n",
    "# x = MaxPooling1D(35)(x)\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "# model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 69, 300)       0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_8 (Convolution1D)  (None, 69, 100)       150100      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 69, 100)       0           convolution1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_8 (MaxPooling1D)    (None, 13, 100)       0           activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_9 (Convolution1D)  (None, 13, 50)        15050       maxpooling1d_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 13, 50)        0           convolution1d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_9 (MaxPooling1D)    (None, 6, 50)         0           activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 300)           0           maxpooling1d_9[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 500)           150500      flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 500)           0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 69)            34569       activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 69)            0           dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 350219\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2739 samples, validate on 305 samples\n",
      "Epoch 1/50\n",
      "2739/2739 [==============================] - 6s - loss: 1.0084 - acc: 0.5111 - val_loss: 1.7087 - val_acc: 0.4393\n",
      "Epoch 2/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.9730 - acc: 0.5144 - val_loss: 1.7393 - val_acc: 0.4098\n",
      "Epoch 3/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.9644 - acc: 0.5210 - val_loss: 1.7671 - val_acc: 0.5115\n",
      "Epoch 4/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.9436 - acc: 0.5206 - val_loss: 1.7462 - val_acc: 0.4525\n",
      "Epoch 5/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.9266 - acc: 0.5133 - val_loss: 1.7611 - val_acc: 0.4426\n",
      "Epoch 6/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.9157 - acc: 0.5225 - val_loss: 1.6720 - val_acc: 0.4656\n",
      "Epoch 7/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8974 - acc: 0.5086 - val_loss: 1.7043 - val_acc: 0.4492\n",
      "Epoch 8/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8983 - acc: 0.5184 - val_loss: 1.7174 - val_acc: 0.4787\n",
      "Epoch 9/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8799 - acc: 0.5144 - val_loss: 1.7682 - val_acc: 0.4689\n",
      "Epoch 10/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8719 - acc: 0.5257 - val_loss: 1.6805 - val_acc: 0.4787\n",
      "Epoch 11/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8706 - acc: 0.5232 - val_loss: 1.7173 - val_acc: 0.5311\n",
      "Epoch 12/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8587 - acc: 0.5232 - val_loss: 1.7160 - val_acc: 0.4754\n",
      "Epoch 13/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8485 - acc: 0.5243 - val_loss: 1.6675 - val_acc: 0.4787\n",
      "Epoch 14/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8422 - acc: 0.5246 - val_loss: 1.7130 - val_acc: 0.4492\n",
      "Epoch 15/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8375 - acc: 0.5312 - val_loss: 1.6819 - val_acc: 0.4590\n",
      "Epoch 16/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8320 - acc: 0.5272 - val_loss: 1.6800 - val_acc: 0.5016\n",
      "Epoch 17/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8333 - acc: 0.5243 - val_loss: 1.6777 - val_acc: 0.5115\n",
      "Epoch 18/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8289 - acc: 0.5254 - val_loss: 1.7079 - val_acc: 0.4525\n",
      "Epoch 19/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8211 - acc: 0.5301 - val_loss: 1.7409 - val_acc: 0.4754\n",
      "Epoch 20/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8172 - acc: 0.5243 - val_loss: 1.7963 - val_acc: 0.4918\n",
      "Epoch 21/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8188 - acc: 0.5257 - val_loss: 1.7417 - val_acc: 0.5377\n",
      "Epoch 22/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8136 - acc: 0.5352 - val_loss: 1.7032 - val_acc: 0.5541\n",
      "Epoch 23/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8082 - acc: 0.5232 - val_loss: 1.7111 - val_acc: 0.5115\n",
      "Epoch 24/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8049 - acc: 0.5279 - val_loss: 1.7088 - val_acc: 0.4393\n",
      "Epoch 25/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8116 - acc: 0.5232 - val_loss: 1.6828 - val_acc: 0.5016\n",
      "Epoch 26/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8004 - acc: 0.5239 - val_loss: 1.7357 - val_acc: 0.4918\n",
      "Epoch 27/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8007 - acc: 0.5345 - val_loss: 1.7691 - val_acc: 0.5049\n",
      "Epoch 28/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.8018 - acc: 0.5287 - val_loss: 1.7832 - val_acc: 0.5279\n",
      "Epoch 29/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7923 - acc: 0.5188 - val_loss: 1.7049 - val_acc: 0.4984\n",
      "Epoch 30/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.8002 - acc: 0.5287 - val_loss: 1.6855 - val_acc: 0.4459\n",
      "Epoch 31/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7897 - acc: 0.5298 - val_loss: 1.7071 - val_acc: 0.4885\n",
      "Epoch 32/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7931 - acc: 0.5433 - val_loss: 1.6833 - val_acc: 0.4951\n",
      "Epoch 33/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7906 - acc: 0.5327 - val_loss: 1.7074 - val_acc: 0.4951\n",
      "Epoch 34/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7886 - acc: 0.5243 - val_loss: 1.7183 - val_acc: 0.5115\n",
      "Epoch 35/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7889 - acc: 0.5385 - val_loss: 1.6925 - val_acc: 0.4590\n",
      "Epoch 36/50\n",
      "2739/2739 [==============================] - 5s - loss: 0.7848 - acc: 0.5345 - val_loss: 1.6418 - val_acc: 0.5246\n",
      "Epoch 37/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7841 - acc: 0.5235 - val_loss: 1.6644 - val_acc: 0.4852\n",
      "Epoch 38/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7809 - acc: 0.5389 - val_loss: 1.7035 - val_acc: 0.5016\n",
      "Epoch 39/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7808 - acc: 0.5312 - val_loss: 1.6619 - val_acc: 0.5016\n",
      "Epoch 40/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7788 - acc: 0.5192 - val_loss: 1.7420 - val_acc: 0.4754\n",
      "Epoch 41/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7834 - acc: 0.5389 - val_loss: 1.7434 - val_acc: 0.4525\n",
      "Epoch 42/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7795 - acc: 0.5301 - val_loss: 1.7121 - val_acc: 0.5115\n",
      "Epoch 43/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7868 - acc: 0.5444 - val_loss: 1.7473 - val_acc: 0.4951\n",
      "Epoch 44/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7844 - acc: 0.5206 - val_loss: 1.7252 - val_acc: 0.4984\n",
      "Epoch 45/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7779 - acc: 0.5345 - val_loss: 1.7483 - val_acc: 0.5082\n",
      "Epoch 46/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7807 - acc: 0.5455 - val_loss: 1.6957 - val_acc: 0.5443\n",
      "Epoch 47/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7708 - acc: 0.5290 - val_loss: 1.7550 - val_acc: 0.5344\n",
      "Epoch 48/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7776 - acc: 0.5341 - val_loss: 1.7071 - val_acc: 0.5148\n",
      "Epoch 49/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7768 - acc: 0.5382 - val_loss: 1.7460 - val_acc: 0.4984\n",
      "Epoch 50/50\n",
      "2739/2739 [==============================] - 6s - loss: 0.7766 - acc: 0.5374 - val_loss: 1.7184 - val_acc: 0.5115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7264008b50>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, train_out,\n",
    "          validation_split=0.1,\n",
    "          batch_size=10,\n",
    "          nb_epoch=50\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
